{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ec54d0",
   "metadata": {},
   "source": [
    "Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n",
    "``requires_grad=True``. This signals to ``autograd`` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95b6c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd051e",
   "metadata": {},
   "source": [
    "We create another tensor Q from a and b.\n",
    "\n",
    "Q=$3a^{3}$ - $b^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a56bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4189947",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4f411",
   "metadata": {},
   "source": [
    "Gradients are now deposited in a.grad and b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059529cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502534b",
   "metadata": {},
   "source": [
    "Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in \n",
    "a directed acyclic graph (DAG) consisting of\n",
    "<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>\n",
    "objects. In this DAG, leaves are the input tensors, roots are the output\n",
    "tensors. By tracing this graph from roots to leaves, you can\n",
    "automatically compute the gradients using the chain rule.\n",
    "\n",
    "**In a forward pass, autograd does two things simultaneously:**\n",
    "\n",
    "- run the requested operation to compute a resulting tensor, and\n",
    "- maintain the operation’s *gradient function* in the DAG.\n",
    "\n",
    "**The backward pass kicks off when ``.backward()`` is called on the DAG\n",
    "root. ``autograd`` then:**\n",
    "\n",
    "- computes the gradients from each ``.grad_fn``,\n",
    "- accumulates them in the respective tensor’s ``.grad`` attribute, and\n",
    "- using the chain rule, propagates all the way to the leaf tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469f8968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12789647",
   "metadata": {},
   "source": [
    "In a NN, parameters that don't compute gradients are usually called **frozen parameters**. \n",
    "It is useful to \"freeze\" part of your model if you know in advance that you won't need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for finetuning a pretrained network \n",
    "<https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html>\n",
    "\n",
    "In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74db8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn, optim\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903773b2",
   "metadata": {},
   "source": [
    "Let's say we want to finetune the model on a new dataset with 10 labels. \n",
    "\n",
    "In resnet, **the classifier is the last linear layer model.fc. \n",
    "We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3c28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4700bc8",
   "metadata": {},
   "source": [
    "Now all parameters in the model, except the parameters of model.fc, are frozen. \n",
    "The only parameters that compute gradients are the weights and bias of model.fc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae5ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca4e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4812574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
